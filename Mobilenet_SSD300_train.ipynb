{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0f42caf4a6c44d8c1ec29e74cd431d3e90f775b4a89b4957cc42fa68a41cd6730",
   "display_name": "Python 3.8.5 64-bit ('mypytorch': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please install libjpeg-turbo and then jpeg4py \n",
    "# https://sourceforge.net/projects/libjpeg-turbo/\n",
    "# pip install jpeg4py\n",
    "# then install prefetch_generator by using pip: pip install prefetch_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "C:\\Users\\Anubhav\\anaconda3\\envs\\mypytorch\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "# conda environments:\n#\nbase                     C:\\Users\\Anubhav\\anaconda3\nacii                     C:\\Users\\Anubhav\\anaconda3\\envs\\acii\namigos                   C:\\Users\\Anubhav\\anaconda3\\envs\\amigos\nideas                    C:\\Users\\Anubhav\\anaconda3\\envs\\ideas\nmypytorch             *  C:\\Users\\Anubhav\\anaconda3\\envs\\mypytorch\nneuralcar                C:\\Users\\Anubhav\\anaconda3\\envs\\neuralcar\nprojectx                 C:\\Users\\Anubhav\\anaconda3\\envs\\projectx\npuzzle                   C:\\Users\\Anubhav\\anaconda3\\envs\\puzzle\n\n"
     ]
    }
   ],
   "source": [
    "!conda env list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jpeg4py as jpeg\n",
    "from PIL import Image\n",
    "\n",
    "# # img = Image.open(r'C:\\Users\\Anubhav\\data\\VOCdevkit\\VOC2007\\JPEGImages\\000001.jpg')\n",
    "# img = jpeg.JPEG(source=r'C:\\Users\\Anubhav\\data\\VOCdevkit\\VOC2007\\JPEGImages\\000001.jpg').decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import *\n",
    "from utils.augmentations import SSDAugmentation\n",
    "from layers.modules import MultiBoxLoss\n",
    "from ssd import build_ssd\n",
    "from ssd_mobilenet import build_ssd_mobilenet\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.init as init\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "from prefetch_generator import BackgroundGenerator\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "class DataLoaderX(data.DataLoader):\n",
    "    def __iter__(self):\n",
    "        return BackgroundGenerator(super().__iter__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_warmup = False\n",
    "args_backbone = 'mobilenet'\n",
    "args_from_scratch = True\n",
    "args_cuda = True\n",
    "args_save_folder = 'weights/'\n",
    "args_gamma = 0.1\n",
    "args_weight_decay = 5e-4\n",
    "args_momentum = 0.9\n",
    "args_lr = 1e-3\n",
    "args_num_workers = 2 # try -1\n",
    "args_start_iter = 0\n",
    "args_batch_size = 32\n",
    "args_dataset_root = VOC_ROOT\n",
    "args_dataset = 'VOC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'C:/Users/Anubhav/Jupyter Projects/computer_vision [mypytorch]/ssd_mobv2/data/VOCdevkit/'"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "VOC_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    if args_cuda:\n",
    "        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "    if not args_cuda:\n",
    "        print(\"WARNING: It looks like you have a CUDA device, but aren't \" +\n",
    "              \"using CUDA.\\nRun with --cuda for optimal training speed.\")\n",
    "        torch.set_default_tensor_type('torch.FloatTensor')\n",
    "else:\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')\n",
    "\n",
    "if not os.path.exists(args_save_folder):\n",
    "    os.mkdir(args_save_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'C:/Users/Anubhav/Jupyter Projects/computer_vision [mypytorch]/ssd_mobv2/'"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup_learning_rate(optimizer, steps, warmup_steps):\n",
    "    min_lr = args_lr / 100\n",
    "    slope = (args_lr - min_lr) / warmup_steps\n",
    "\n",
    "    lr = steps * slope + min_lr\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "def adjust_learning_rate(optimizer, gamma, step):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 at every\n",
    "        specified step\n",
    "    # Adapted from PyTorch Imagenet example:\n",
    "    # https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
    "    \"\"\"\n",
    "    lr = args_lr * (gamma ** (step))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "def xavier(param):\n",
    "    init.xavier_uniform(param)\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        xavier(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "C:/Users/Anubhav/Jupyter Projects/computer_vision [mypytorch]/ssd_mobv2/\n",
      "C:/Users/Anubhav/Jupyter Projects/computer_vision [mypytorch]/ssd_mobv2/\n",
      "Initializing weights...\n",
      "SSDMobileNetV2(\n",
      "  (backbone): ModuleList(\n",
      "    (0): ConvBNReLU(\n",
      "      (0): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): CELU(alpha=1.0, inplace=True)\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): CELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
      "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): CELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(24, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): CELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): CELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): CELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): CELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): CELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): CELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): CELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): CELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): CELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): CELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): CELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): CELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): CELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)\n",
      "          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): CELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (9): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): CELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)\n",
      "          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): CELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (10): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): CELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)\n",
      "          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): CELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (11): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): CELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)\n",
      "          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): CELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (12): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): Conv2d(72, 432, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): CELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): Conv2d(432, 432, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=432, bias=False)\n",
      "          (1): BatchNorm2d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): CELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(432, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (13): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): Conv2d(72, 432, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): CELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): Conv2d(432, 432, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=432, bias=False)\n",
      "          (1): BatchNorm2d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): CELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(432, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (14): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): Conv2d(72, 432, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): CELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): Conv2d(432, 432, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=432, bias=False)\n",
      "          (1): BatchNorm2d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): CELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(432, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (15): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): Conv2d(120, 720, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): CELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): Conv2d(720, 720, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=720, bias=False)\n",
      "          (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): CELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(720, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (16): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): Conv2d(120, 720, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): CELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): Conv2d(720, 720, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=720, bias=False)\n",
      "          (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): CELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(720, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (17): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): Conv2d(120, 720, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): CELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): Conv2d(720, 720, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=720, bias=False)\n",
      "          (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): CELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(720, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): L2Norm()\n",
      "  (extras): ModuleList(\n",
      "    (0): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): Conv2d(240, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): CELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): CELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): CELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): CELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): CELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): CELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): CELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): CELU(alpha=1.0, inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (loc): ModuleList(\n",
      "    (0): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (2): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (conf): ModuleList(\n",
      "    (0): Conv2d(256, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Conv2d(256, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (2): Conv2d(256, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): Conv2d(256, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): Conv2d(256, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): Conv2d(256, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (top_down): ModuleList(\n",
      "    (0): Conv2d(72, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): Conv2d(240, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (2): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (5): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (final_features): ModuleList(\n",
      "    (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (5): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n",
      "extras: ModuleList(\n",
      "  (0): InvertedResidual(\n",
      "    (conv): Sequential(\n",
      "      (0): ConvBNReLU(\n",
      "        (0): Conv2d(240, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): CELU(alpha=1.0, inplace=True)\n",
      "      )\n",
      "      (1): ConvBNReLU(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): CELU(alpha=1.0, inplace=True)\n",
      "      )\n",
      "      (2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): InvertedResidual(\n",
      "    (conv): Sequential(\n",
      "      (0): ConvBNReLU(\n",
      "        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): CELU(alpha=1.0, inplace=True)\n",
      "      )\n",
      "      (1): ConvBNReLU(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): CELU(alpha=1.0, inplace=True)\n",
      "      )\n",
      "      (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (2): InvertedResidual(\n",
      "    (conv): Sequential(\n",
      "      (0): ConvBNReLU(\n",
      "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): CELU(alpha=1.0, inplace=True)\n",
      "      )\n",
      "      (1): ConvBNReLU(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): CELU(alpha=1.0, inplace=True)\n",
      "      )\n",
      "      (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (3): InvertedResidual(\n",
      "    (conv): Sequential(\n",
      "      (0): ConvBNReLU(\n",
      "        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): CELU(alpha=1.0, inplace=True)\n",
      "      )\n",
      "      (1): ConvBNReLU(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): CELU(alpha=1.0, inplace=True)\n",
      "      )\n",
      "      (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Loading the dataset...\n",
      "Training SSD on: VOC\n",
      "c:\\Users\\Anubhav\\Jupyter Projects\\computer_vision [mypytorch]\\ssd_mobv2\\ssd_mobilenet.py:34: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  self.priors = Variable(self.priorbox.forward(), volatile=True)\n",
      "<ipython-input-14-1926969fad54>:20: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  init.xavier_uniform(param)\n"
     ]
    }
   ],
   "source": [
    "''' config files contains  HOME path, COLORS, MEANS, voc config'''\n",
    "cfg = voc\n",
    "dataset = VOCDetection(root=args_dataset_root, transform=SSDAugmentation(cfg['min_dim'], MEANS))\n",
    "\n",
    "ssd_net = build_ssd_mobilenet('train', cfg)\n",
    "\n",
    "print('Initializing weights...')\n",
    "# initialize newly added layers' weights with xavier method\n",
    "ssd_net.extras.apply(weights_init)\n",
    "ssd_net.loc.apply(weights_init)\n",
    "ssd_net.conf.apply(weights_init)\n",
    "\n",
    "net = ssd_net\n",
    "print(net)\n",
    "print('extras:', net.extras)\n",
    "\n",
    "if args_cuda:\n",
    "    net = torch.nn.DataParallel(ssd_net)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "if args_cuda:\n",
    "    net = net.cuda()\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=args_lr, momentum=args_momentum,\n",
    "                        weight_decay=args_weight_decay)\n",
    "scheduler = lr_scheduler.CosineAnnealingLR(optimizer, 100)\n",
    "criterion = MultiBoxLoss(cfg['num_classes'], 0.4, True, 0, True, 3, 0.5,\n",
    "                            False, args_cuda)\n",
    "\n",
    "net.train()\n",
    "# loss counters\n",
    "loc_loss = 0\n",
    "conf_loss = 0\n",
    "epoch = 0\n",
    "\n",
    "print('Loading the dataset...')\n",
    "\n",
    "epoch_size = len(dataset) // args_batch_size\n",
    "print('Training SSD on:', args_dataset)\n",
    "\n",
    "step_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoaderX(dataset, args_batch_size,\n",
    "                                num_workers=args_num_workers,\n",
    "                                shuffle=True, collate_fn=detection_collate,\n",
    "                                pin_memory=True)\n",
    "# create batch iterator\n",
    "batch_iterator = iter(data_loader)\n",
    "t0 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Anubhav\\anaconda3\\envs\\mypytorch\\lib\\site-packages\\torch\\nn\\functional.py:2952: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
      "C:\\Users\\Anubhav\\anaconda3\\envs\\mypytorch\\lib\\site-packages\\torch\\nn\\_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "timer: 420.6728 sec.\n",
      "iter 1000 || Loc loss: 0.0435 || Conf loss: 0.0355 || lr: 0.001000 timer: 411.3912 sec.\n",
      "iter 2000 || Loc loss: 0.0741 || Conf loss: 0.0666 || lr: 0.001000 timer: 400.3740 sec.\n",
      "iter 3000 || Loc loss: 0.0217 || Conf loss: 0.0205 || lr: 0.001000 timer: 402.8619 sec.\n",
      "iter 4000 || Loc loss: 0.0531 || Conf loss: 0.0514 || lr: 0.001000 timer: 402.2565 sec.\n",
      "iter 5000 || Loc loss: 0.0058 || Conf loss: 0.0065 || lr: 0.001000 Saving state, iter: 5000\n",
      "timer: 400.4428 sec.\n",
      "iter 6000 || Loc loss: 0.0337 || Conf loss: 0.0367 || lr: 0.001000 timer: 396.4493 sec.\n",
      "iter 7000 || Loc loss: 0.0612 || Conf loss: 0.0670 || lr: 0.001000 timer: 390.1533 sec.\n",
      "iter 8000 || Loc loss: 0.0212 || Conf loss: 0.0233 || lr: 0.001000 timer: 396.7545 sec.\n",
      "iter 9000 || Loc loss: 0.0454 || Conf loss: 0.0527 || lr: 0.001000 C:\\Users\\Anubhav\\anaconda3\\envs\\mypytorch\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "timer: 390.5825 sec.\n",
      "iter 10000 || Loc loss: 0.0077 || Conf loss: 0.0100 || lr: 0.000854 Saving state, iter: 10000\n",
      "timer: 399.2013 sec.\n",
      "iter 11000 || Loc loss: 0.0312 || Conf loss: 0.0386 || lr: 0.000831 timer: 396.2680 sec.\n",
      "iter 12000 || Loc loss: 0.0524 || Conf loss: 0.0652 || lr: 0.000794 timer: 400.3190 sec.\n",
      "iter 13000 || Loc loss: 0.0203 || Conf loss: 0.0261 || lr: 0.000768 timer: 400.8705 sec.\n",
      "iter 14000 || Loc loss: 0.0409 || Conf loss: 0.0529 || lr: 0.000727 timer: 393.9360 sec.\n",
      "iter 15000 || Loc loss: 0.0102 || Conf loss: 0.0134 || lr: 0.000699 Saving state, iter: 15000\n",
      "timer: 395.2349 sec.\n",
      "iter 16000 || Loc loss: 0.0305 || Conf loss: 0.0400 || lr: 0.000655 timer: 390.5150 sec.\n",
      "iter 17000 || Loc loss: 0.0013 || Conf loss: 0.0019 || lr: 0.000624 timer: 398.3468 sec.\n",
      "iter 18000 || Loc loss: 0.0207 || Conf loss: 0.0276 || lr: 0.000578 timer: 391.1147 sec.\n",
      "iter 19000 || Loc loss: 0.0379 || Conf loss: 0.0517 || lr: 0.000547 timer: 395.5072 sec.\n",
      "iter 20000 || Loc loss: 0.0113 || Conf loss: 0.0162 || lr: 0.000500 Saving state, iter: 20000\n",
      "timer: 397.8941 sec.\n",
      "iter 21000 || Loc loss: 0.0288 || Conf loss: 0.0400 || lr: 0.000469 timer: 393.4266 sec.\n",
      "iter 22000 || Loc loss: 0.0039 || Conf loss: 0.0056 || lr: 0.000422 timer: 396.2721 sec.\n",
      "iter 23000 || Loc loss: 0.0202 || Conf loss: 0.0285 || lr: 0.000391 timer: 392.5096 sec.\n",
      "iter 24000 || Loc loss: 0.0359 || Conf loss: 0.0520 || lr: 0.000345 timer: 396.8836 sec.\n",
      "iter 25000 || Loc loss: 0.0119 || Conf loss: 0.0177 || lr: 0.000316 Saving state, iter: 25000\n",
      "timer: 392.5906 sec.\n",
      "iter 26000 || Loc loss: 0.0270 || Conf loss: 0.0406 || lr: 0.000273 timer: 395.0945 sec.\n",
      "iter 27000 || Loc loss: 0.0056 || Conf loss: 0.0080 || lr: 0.000245 timer: 396.6441 sec.\n",
      "iter 28000 || Loc loss: 0.0209 || Conf loss: 0.0308 || lr: 0.000206 timer: 391.7939 sec.\n",
      "iter 29000 || Loc loss: 0.0374 || Conf loss: 0.0532 || lr: 0.000181 timer: 396.7096 sec.\n",
      "iter 30000 || Loc loss: 0.0136 || Conf loss: 0.0191 || lr: 0.000146 Saving state, iter: 30000\n",
      "timer: 392.2133 sec.\n",
      "iter 31000 || Loc loss: 0.0279 || Conf loss: 0.0418 || lr: 0.000125 timer: 402.4052 sec.\n",
      "iter 32000 || Loc loss: 0.0064 || Conf loss: 0.0100 || lr: 0.000095 timer: 394.2965 sec.\n",
      "iter 33000 || Loc loss: 0.0211 || Conf loss: 0.0322 || lr: 0.000078 timer: 401.1941 sec.\n",
      "iter 34000 || Loc loss: 0.0009 || Conf loss: 0.0014 || lr: 0.000054 timer: 398.6621 sec.\n",
      "iter 35000 || Loc loss: 0.0147 || Conf loss: 0.0218 || lr: 0.000041 Saving state, iter: 35000\n",
      "timer: 392.1591 sec.\n",
      "iter 36000 || Loc loss: 0.0286 || Conf loss: 0.0439 || lr: 0.000024 timer: 396.9517 sec.\n",
      "iter 37000 || Loc loss: 0.0091 || Conf loss: 0.0132 || lr: 0.000016 timer: 391.2294 sec.\n",
      "iter 38000 || Loc loss: 0.0207 || Conf loss: 0.0334 || lr: 0.000006 timer: 396.0446 sec.\n",
      "iter 39000 || Loc loss: 0.0028 || Conf loss: 0.0044 || lr: 0.000002 timer: 392.2615 sec.\n",
      "iter 40000 || Loc loss: 0.0167 || Conf loss: 0.0249 || lr: 0.000000 Saving state, iter: 40000\n",
      "timer: 397.1176 sec.\n",
      "iter 41000 || Loc loss: 0.0292 || Conf loss: 0.0449 || lr: 0.000001 timer: 396.6258 sec.\n",
      "iter 42000 || Loc loss: 0.0110 || Conf loss: 0.0167 || lr: 0.000006 timer: 392.1450 sec.\n",
      "iter 43000 || Loc loss: 0.0252 || Conf loss: 0.0384 || lr: 0.000012 timer: 396.8481 sec.\n",
      "iter 44000 || Loc loss: 0.0050 || Conf loss: 0.0076 || lr: 0.000024 timer: 392.5186 sec.\n",
      "iter 45000 || Loc loss: 0.0201 || Conf loss: 0.0299 || lr: 0.000035 Saving state, iter: 45000\n",
      "timer: 397.7248 sec.\n",
      "iter 46000 || Loc loss: 0.0313 || Conf loss: 0.0491 || lr: 0.000054 timer: 392.5521 sec.\n",
      "iter 47000 || Loc loss: 0.0124 || Conf loss: 0.0192 || lr: 0.000070 timer: 399.3081 sec.\n",
      "iter 48000 || Loc loss: 0.0248 || Conf loss: 0.0396 || lr: 0.000095 timer: 398.3729 sec.\n",
      "iter 49000 || Loc loss: 0.0069 || Conf loss: 0.0106 || lr: 0.000115 timer: 396.6753 sec.\n",
      "iter 50000 || Loc loss: 0.0199 || Conf loss: 0.0310 || lr: 0.000146 Saving state, iter: 50000\n",
      "timer: 401.9723 sec.\n",
      "iter 51000 || Loc loss: 0.0006 || Conf loss: 0.0013 || lr: 0.000169 timer: 393.1337 sec.\n",
      "iter 52000 || Loc loss: 0.0155 || Conf loss: 0.0230 || lr: 0.000206 timer: 400.0116 sec.\n",
      "iter 53000 || Loc loss: 0.0289 || Conf loss: 0.0433 || lr: 0.000232 timer: 394.3976 sec.\n",
      "iter 54000 || Loc loss: 0.0095 || Conf loss: 0.0140 || lr: 0.000273 timer: 398.1097 sec.\n",
      "iter 55000 || Loc loss: 0.0235 || Conf loss: 0.0353 || lr: 0.000301 Saving state, iter: 55000\n",
      "timer: 401.0175 sec.\n",
      "iter 56000 || Loc loss: 0.0028 || Conf loss: 0.0042 || lr: 0.000345 timer: 393.7988 sec.\n",
      "iter 57000 || Loc loss: 0.0181 || Conf loss: 0.0260 || lr: 0.000376 timer: 399.5350 sec.\n",
      "iter 58000 || Loc loss: 0.0326 || Conf loss: 0.0476 || lr: 0.000422 timer: 392.4789 sec.\n",
      "iter 59000 || Loc loss: 0.0119 || Conf loss: 0.0171 || lr: 0.000453 timer: 401.2365 sec.\n",
      "iter 60000 || Loc loss: 0.0272 || Conf loss: 0.0398 || lr: 0.000500 Saving state, iter: 60000\n",
      "timer: 395.5040 sec.\n",
      "iter 61000 || Loc loss: 0.0047 || Conf loss: 0.0072 || lr: 0.000531 timer: 400.3192 sec.\n",
      "iter 62000 || Loc loss: 0.0192 || Conf loss: 0.0300 || lr: 0.000578 timer: 418.2478 sec.\n",
      "iter 63000 || Loc loss: 0.0329 || Conf loss: 0.0501 || lr: 0.000609 timer: 398.4954 sec.\n",
      "iter 64000 || Loc loss: 0.0133 || Conf loss: 0.0199 || lr: 0.000655 timer: 453.8717 sec.\n",
      "iter 65000 || Loc loss: 0.0292 || Conf loss: 0.0420 || lr: 0.000684 Saving state, iter: 65000\n",
      "timer: 392.8217 sec.\n",
      "iter 66000 || Loc loss: 0.0065 || Conf loss: 0.0106 || lr: 0.000727 timer: 398.3034 sec.\n",
      "iter 67000 || Loc loss: 0.0202 || Conf loss: 0.0314 || lr: 0.000755 timer: 393.9658 sec.\n",
      "iter 68000 || Loc loss: 0.0010 || Conf loss: 0.0016 || lr: 0.000794 timer: 397.3749 sec.\n",
      "iter 69000 || Loc loss: 0.0138 || Conf loss: 0.0212 || lr: 0.000819 timer: 398.6584 sec.\n",
      "iter 70000 || Loc loss: 0.0276 || Conf loss: 0.0424 || lr: 0.000854 Saving state, iter: 70000\n",
      "timer: 391.8750 sec.\n",
      "iter 71000 || Loc loss: 0.0089 || Conf loss: 0.0138 || lr: 0.000875 timer: 397.5516 sec.\n",
      "iter 72000 || Loc loss: 0.0228 || Conf loss: 0.0347 || lr: 0.000905 timer: 392.8987 sec.\n",
      "iter 73000 || Loc loss: 0.0023 || Conf loss: 0.0041 || lr: 0.000922 timer: 398.2958 sec.\n",
      "iter 74000 || Loc loss: 0.0166 || Conf loss: 0.0247 || lr: 0.000946 timer: 393.6232 sec.\n",
      "iter 75000 || Loc loss: 0.0290 || Conf loss: 0.0446 || lr: 0.000959 Saving state, iter: 75000\n",
      "timer: 399.7834 sec.\n",
      "iter 76000 || Loc loss: 0.0096 || Conf loss: 0.0153 || lr: 0.000976 timer: 399.7237 sec.\n",
      "iter 77000 || Loc loss: 0.0230 || Conf loss: 0.0357 || lr: 0.000984 timer: 391.8680 sec.\n",
      "iter 78000 || Loc loss: 0.0041 || Conf loss: 0.0070 || lr: 0.000994 timer: 397.7213 sec.\n",
      "iter 79000 || Loc loss: 0.0169 || Conf loss: 0.0269 || lr: 0.000998 timer: 392.3759 sec.\n",
      "iter 80000 || Loc loss: 0.0288 || Conf loss: 0.0467 || lr: 0.001000 Saving state, iter: 80000\n",
      "timer: 397.8498 sec.\n",
      "iter 81000 || Loc loss: 0.0113 || Conf loss: 0.0174 || lr: 0.000999 timer: 447.6078 sec.\n",
      "iter 82000 || Loc loss: 0.0239 || Conf loss: 0.0373 || lr: 0.000994 timer: 401.8291 sec.\n",
      "iter 83000 || Loc loss: 0.0055 || Conf loss: 0.0092 || lr: 0.000988 timer: 401.5074 sec.\n",
      "iter 84000 || Loc loss: 0.0166 || Conf loss: 0.0270 || lr: 0.000976 timer: 431.2808 sec.\n",
      "iter 85000 || Loc loss: 0.0009 || Conf loss: 0.0015 || lr: 0.000965 Saving state, iter: 85000\n",
      "timer: 418.1962 sec.\n",
      "iter 86000 || Loc loss: 0.0119 || Conf loss: 0.0189 || lr: 0.000946 timer: 414.3912 sec.\n",
      "iter 87000 || Loc loss: 0.0221 || Conf loss: 0.0362 || lr: 0.000930 timer: 419.8649 sec.\n",
      "iter 88000 || Loc loss: 0.0070 || Conf loss: 0.0113 || lr: 0.000905 timer: 411.3867 sec.\n",
      "iter 89000 || Loc loss: 0.0167 || Conf loss: 0.0285 || lr: 0.000885 timer: 414.5506 sec.\n",
      "iter 90000 || Loc loss: 0.0030 || Conf loss: 0.0044 || lr: 0.000854 Saving state, iter: 90000\n",
      "timer: 415.9726 sec.\n",
      "iter 91000 || Loc loss: 0.0136 || Conf loss: 0.0209 || lr: 0.000831 timer: 399.0261 sec.\n",
      "iter 92000 || Loc loss: 0.0227 || Conf loss: 0.0374 || lr: 0.000794 timer: 397.7238 sec.\n",
      "iter 93000 || Loc loss: 0.0080 || Conf loss: 0.0135 || lr: 0.000768 timer: 393.1058 sec.\n",
      "iter 94000 || Loc loss: 0.0171 || Conf loss: 0.0291 || lr: 0.000727 timer: 397.4199 sec.\n",
      "iter 95000 || Loc loss: 0.0036 || Conf loss: 0.0062 || lr: 0.000699 Saving state, iter: 95000\n",
      "timer: 391.6747 sec.\n",
      "iter 96000 || Loc loss: 0.0135 || Conf loss: 0.0225 || lr: 0.000655 timer: 397.2433 sec.\n",
      "iter 97000 || Loc loss: 0.0228 || Conf loss: 0.0388 || lr: 0.000624 timer: 397.6994 sec.\n",
      "iter 98000 || Loc loss: 0.0083 || Conf loss: 0.0147 || lr: 0.000578 timer: 390.7593 sec.\n",
      "iter 99000 || Loc loss: 0.0171 || Conf loss: 0.0296 || lr: 0.000547 timer: 405.5707 sec.\n",
      "iter 100000 || Loc loss: 0.0049 || Conf loss: 0.0083 || lr: 0.000500 Saving state, iter: 100000\n",
      "timer: 408.3126 sec.\n",
      "iter 101000 || Loc loss: 0.0119 || Conf loss: 0.0225 || lr: 0.000469 timer: 426.3792 sec.\n",
      "iter 102000 || Loc loss: 0.0006 || Conf loss: 0.0010 || lr: 0.000422 timer: 412.5491 sec.\n",
      "iter 103000 || Loc loss: 0.0099 || Conf loss: 0.0167 || lr: 0.000391 timer: 412.7689 sec.\n",
      "iter 104000 || Loc loss: 0.0155 || Conf loss: 0.0285 || lr: 0.000345 timer: 417.4823 sec.\n",
      "iter 105000 || Loc loss: 0.0064 || Conf loss: 0.0103 || lr: 0.000316 Saving state, iter: 105000\n",
      "timer: 409.8007 sec.\n",
      "iter 106000 || Loc loss: 0.0125 || Conf loss: 0.0230 || lr: 0.000273 timer: 417.9204 sec.\n",
      "iter 107000 || Loc loss: 0.0017 || Conf loss: 0.0031 || lr: 0.000245 timer: 415.7037 sec.\n",
      "iter 108000 || Loc loss: 0.0085 || Conf loss: 0.0165 || lr: 0.000206 timer: 417.8779 sec.\n",
      "iter 109000 || Loc loss: 0.0169 || Conf loss: 0.0296 || lr: 0.000181 timer: 413.5767 sec.\n",
      "iter 110000 || Loc loss: 0.0056 || Conf loss: 0.0108 || lr: 0.000146 Saving state, iter: 110000\n",
      "timer: 417.2698 sec.\n",
      "iter 111000 || Loc loss: 0.0127 || Conf loss: 0.0234 || lr: 0.000125 timer: 413.4480 sec.\n",
      "iter 112000 || Loc loss: 0.0032 || Conf loss: 0.0058 || lr: 0.000095 timer: 426.2965 sec.\n",
      "iter 113000 || Loc loss: 0.0104 || Conf loss: 0.0181 || lr: 0.000078 timer: 418.6508 sec.\n",
      "iter 114000 || Loc loss: 0.0171 || Conf loss: 0.0311 || lr: 0.000054 timer: 412.3835 sec.\n",
      "iter 115000 || Loc loss: 0.0066 || Conf loss: 0.0122 || lr: 0.000041 Saving state, iter: 115000\n",
      "timer: 412.8050 sec.\n",
      "iter 116000 || Loc loss: 0.0127 || Conf loss: 0.0242 || lr: 0.000024 timer: 394.2740 sec.\n",
      "iter 117000 || Loc loss: 0.0035 || Conf loss: 0.0067 || lr: 0.000016 timer: 400.4066 sec.\n",
      "iter 118000 || Loc loss: 0.0110 || Conf loss: 0.0207 || lr: 0.000006 timer: 401.4806 sec.\n",
      "iter 119000 || Loc loss: 0.0004 || Conf loss: 0.0007 || lr: 0.000002 timer: 394.6626 sec.\n",
      "iter 120000 || Loc loss: 0.0076 || Conf loss: 0.0140 || lr: 0.000000 Saving state, iter: 120000\n",
      "timer: 406.6211 sec.\n",
      "iter 121000 || Loc loss: 0.0140 || Conf loss: 0.0263 || lr: 0.000001 timer: 402.4449 sec.\n",
      "iter 122000 || Loc loss: 0.0046 || Conf loss: 0.0081 || lr: 0.000006 timer: 406.8716 sec.\n",
      "iter 123000 || Loc loss: 0.0107 || Conf loss: 0.0210 || lr: 0.000012 timer: 402.6929 sec.\n",
      "iter 124000 || Loc loss: 0.0017 || Conf loss: 0.0029 || lr: 0.000024 timer: 415.0459 sec.\n",
      "iter 125000 || Loc loss: 0.0086 || Conf loss: 0.0159 || lr: 0.000035 Saving state, iter: 125000\n",
      "timer: 477.5753 sec.\n",
      "iter 126000 || Loc loss: 0.0151 || Conf loss: 0.0288 || lr: 0.000054 timer: 412.1769 sec.\n",
      "iter 127000 || Loc loss: 0.0054 || Conf loss: 0.0100 || lr: 0.000070 timer: 409.4050 sec.\n",
      "iter 128000 || Loc loss: 0.0125 || Conf loss: 0.0234 || lr: 0.000095 timer: 420.1288 sec.\n",
      "iter 129000 || Loc loss: 0.0029 || Conf loss: 0.0051 || lr: 0.000115 timer: 478.0145 sec.\n",
      "iter 130000 || Loc loss: 0.0095 || Conf loss: 0.0175 || lr: 0.000146 Saving state, iter: 130000\n",
      "timer: 412.7476 sec.\n",
      "iter 131000 || Loc loss: 0.0173 || Conf loss: 0.0321 || lr: 0.000169 timer: 421.1791 sec.\n",
      "iter 132000 || Loc loss: 0.0068 || Conf loss: 0.0120 || lr: 0.000206 timer: 418.9966 sec.\n",
      "iter 133000 || Loc loss: 0.0144 || Conf loss: 0.0267 || lr: 0.000232 timer: 414.3303 sec.\n",
      "iter 134000 || Loc loss: 0.0035 || Conf loss: 0.0063 || lr: 0.000273 timer: 417.0806 sec.\n",
      "iter 135000 || Loc loss: 0.0108 || Conf loss: 0.0207 || lr: 0.000301 Saving state, iter: 135000\n",
      "timer: 408.7916 sec.\n",
      "iter 136000 || Loc loss: 0.0007 || Conf loss: 0.0012 || lr: 0.000345 timer: 413.1655 sec.\n",
      "iter 137000 || Loc loss: 0.0092 || Conf loss: 0.0158 || lr: 0.000376 timer: 396.2067 sec.\n",
      "iter 138000 || Loc loss: 0.0159 || Conf loss: 0.0287 || lr: 0.000422 timer: 399.6604 sec.\n",
      "iter 139000 || Loc loss: 0.0045 || Conf loss: 0.0089 || lr: 0.000453 timer: 399.3617 sec.\n",
      "iter 140000 || Loc loss: 0.0132 || Conf loss: 0.0245 || lr: 0.000500 Saving state, iter: 140000\n",
      "timer: 393.3885 sec.\n",
      "iter 141000 || Loc loss: 0.0020 || Conf loss: 0.0033 || lr: 0.000531 timer: 402.8028 sec.\n",
      "iter 142000 || Loc loss: 0.0114 || Conf loss: 0.0188 || lr: 0.000578 timer: 393.3026 sec.\n",
      "iter 143000 || Loc loss: 0.0192 || Conf loss: 0.0328 || lr: 0.000609 timer: 399.2620 sec.\n",
      "iter 144000 || Loc loss: 0.0069 || Conf loss: 0.0120 || lr: 0.000655 timer: 397.8789 sec.\n",
      "iter 145000 || Loc loss: 0.0151 || Conf loss: 0.0270 || lr: 0.000684 Saving state, iter: 145000\n",
      "timer: 411.9210 sec.\n",
      "iter 146000 || Loc loss: 0.0029 || Conf loss: 0.0052 || lr: 0.000727 timer: 408.0451 sec.\n",
      "iter 147000 || Loc loss: 0.0128 || Conf loss: 0.0223 || lr: 0.000755 timer: 410.8891 sec.\n",
      "iter 148000 || Loc loss: 0.0205 || Conf loss: 0.0359 || lr: 0.000794 timer: 413.4741 sec.\n",
      "iter 149000 || Loc loss: 0.0081 || Conf loss: 0.0147 || lr: 0.000819 timer: 408.7559 sec.\n",
      "iter 150000 || Loc loss: 0.0174 || Conf loss: 0.0299 || lr: 0.000854 Saving state, iter: 150000\n",
      "timer: 452.4012 sec.\n",
      "iter 151000 || Loc loss: 0.0044 || Conf loss: 0.0079 || lr: 0.000875 timer: 408.5089 sec.\n",
      "iter 152000 || Loc loss: 0.0144 || Conf loss: 0.0237 || lr: 0.000905 timer: 414.1978 sec.\n",
      "iter 153000 || Loc loss: 0.0005 || Conf loss: 0.0010 || lr: 0.000922 timer: 413.1876 sec.\n",
      "iter 154000 || Loc loss: 0.0102 || Conf loss: 0.0171 || lr: 0.000946 timer: 407.6513 sec.\n",
      "iter 155000 || Loc loss: 0.0200 || Conf loss: 0.0337 || lr: 0.000959 Saving state, iter: 155000\n",
      "timer: 402.0129 sec.\n",
      "iter 156000 || Loc loss: 0.0060 || Conf loss: 0.0101 || lr: 0.000976 timer: 394.5014 sec.\n",
      "iter 157000 || Loc loss: 0.0143 || Conf loss: 0.0252 || lr: 0.000984 timer: 397.8676 sec.\n",
      "iter 158000 || Loc loss: 0.0021 || Conf loss: 0.0036 || lr: 0.000994 timer: 393.6582 sec.\n",
      "iter 159000 || Loc loss: 0.0109 || Conf loss: 0.0189 || lr: 0.000998 timer: 418.0312 sec.\n",
      "iter 160000 || Loc loss: 0.0198 || Conf loss: 0.0344 || lr: 0.001000 Saving state, iter: 160000\n",
      "timer: 404.5550 sec.\n",
      "iter 161000 || Loc loss: 0.0075 || Conf loss: 0.0133 || lr: 0.000999 timer: 406.0234 sec.\n",
      "iter 162000 || Loc loss: 0.0167 || Conf loss: 0.0288 || lr: 0.000994 timer: 420.7117 sec.\n",
      "iter 163000 || Loc loss: 0.0034 || Conf loss: 0.0059 || lr: 0.000988 timer: 409.9240 sec.\n",
      "iter 164000 || Loc loss: 0.0120 || Conf loss: 0.0208 || lr: 0.000976 timer: 401.9122 sec.\n",
      "iter 165000 || Loc loss: 0.0224 || Conf loss: 0.0388 || lr: 0.000965 Saving state, iter: 165000\n",
      "timer: 400.1211 sec.\n",
      "iter 166000 || Loc loss: 0.0080 || Conf loss: 0.0139 || lr: 0.000946 timer: 418.5756 sec.\n",
      "iter 167000 || Loc loss: 0.0162 || Conf loss: 0.0287 || lr: 0.000930 timer: 455.2115 sec.\n",
      "iter 168000 || Loc loss: 0.0048 || Conf loss: 0.0080 || lr: 0.000905 timer: 413.5008 sec.\n",
      "iter 169000 || Loc loss: 0.0128 || Conf loss: 0.0221 || lr: 0.000885 timer: 418.2329 sec.\n",
      "iter 170000 || Loc loss: 0.0009 || Conf loss: 0.0015 || lr: 0.000854 Saving state, iter: 170000\n",
      "timer: 427.3458 sec.\n",
      "iter 171000 || Loc loss: 0.0076 || Conf loss: 0.0144 || lr: 0.000831 timer: 422.5756 sec.\n",
      "iter 172000 || Loc loss: 0.0167 || Conf loss: 0.0306 || lr: 0.000794 timer: 404.2268 sec.\n",
      "iter 173000 || Loc loss: 0.0052 || Conf loss: 0.0090 || lr: 0.000768 timer: 408.3417 sec.\n",
      "iter 174000 || Loc loss: 0.0135 || Conf loss: 0.0232 || lr: 0.000727 timer: 416.0035 sec.\n",
      "iter 175000 || Loc loss: 0.0015 || Conf loss: 0.0031 || lr: 0.000699 Saving state, iter: 175000\n",
      "timer: 442.8841 sec.\n",
      "iter 176000 || Loc loss: 0.0085 || Conf loss: 0.0162 || lr: 0.000655 timer: 416.0211 sec.\n",
      "iter 177000 || Loc loss: 0.0146 || Conf loss: 0.0287 || lr: 0.000624 timer: 439.6904 sec.\n",
      "iter 178000 || Loc loss: 0.0050 || Conf loss: 0.0096 || lr: 0.000578 timer: 428.6231 sec.\n",
      "iter 179000 || Loc loss: 0.0129 || Conf loss: 0.0234 || lr: 0.000547 timer: 409.5486 sec.\n",
      "iter 180000 || Loc loss: 0.0022 || Conf loss: 0.0044 || lr: 0.000500 Saving state, iter: 180000\n",
      "timer: 418.6851 sec.\n",
      "iter 181000 || Loc loss: 0.0095 || Conf loss: 0.0179 || lr: 0.000469 timer: 423.5551 sec.\n",
      "iter 182000 || Loc loss: 0.0156 || Conf loss: 0.0298 || lr: 0.000422 timer: 409.1146 sec.\n",
      "iter 183000 || Loc loss: 0.0063 || Conf loss: 0.0117 || lr: 0.000391 timer: 416.1345 sec.\n",
      "iter 184000 || Loc loss: 0.0124 || Conf loss: 0.0239 || lr: 0.000345 timer: 404.0410 sec.\n",
      "iter 185000 || Loc loss: 0.0033 || Conf loss: 0.0060 || lr: 0.000316 Saving state, iter: 185000\n",
      "timer: 409.6237 sec.\n",
      "iter 186000 || Loc loss: 0.0094 || Conf loss: 0.0177 || lr: 0.000273 timer: 394.7608 sec.\n",
      "iter 187000 || Loc loss: 0.0005 || Conf loss: 0.0012 || lr: 0.000245 timer: 404.6187 sec.\n",
      "iter 188000 || Loc loss: 0.0063 || Conf loss: 0.0128 || lr: 0.000206 timer: 406.9396 sec.\n",
      "iter 189000 || Loc loss: 0.0122 || Conf loss: 0.0233 || lr: 0.000181 timer: 403.3759 sec.\n",
      "iter 190000 || Loc loss: 0.0031 || Conf loss: 0.0067 || lr: 0.000146 Saving state, iter: 190000\n",
      "timer: 409.3742 sec.\n",
      "iter 191000 || Loc loss: 0.0083 || Conf loss: 0.0179 || lr: 0.000125 timer: 439.1196 sec.\n",
      "iter 192000 || Loc loss: 0.0010 || Conf loss: 0.0021 || lr: 0.000095 timer: 457.3865 sec.\n",
      "iter 193000 || Loc loss: 0.0067 || Conf loss: 0.0132 || lr: 0.000078 timer: 633.0975 sec.\n",
      "iter 194000 || Loc loss: 0.0124 || Conf loss: 0.0247 || lr: 0.000054 timer: 535.8880 sec.\n",
      "iter 195000 || Loc loss: 0.0045 || Conf loss: 0.0087 || lr: 0.000041 Saving state, iter: 195000\n",
      "timer: 412.0100 sec.\n",
      "iter 196000 || Loc loss: 0.0104 || Conf loss: 0.0203 || lr: 0.000024 timer: 407.1382 sec.\n",
      "iter 197000 || Loc loss: 0.0017 || Conf loss: 0.0039 || lr: 0.000016 timer: 415.7118 sec.\n",
      "iter 198000 || Loc loss: 0.0073 || Conf loss: 0.0148 || lr: 0.000006 timer: 409.4775 sec.\n",
      "iter 199000 || Loc loss: 0.0132 || Conf loss: 0.0260 || lr: 0.000002 timer: 405.3900 sec.\n",
      "iter 200000 || Loc loss: 0.0045 || Conf loss: 0.0098 || lr: 0.000000 Saving state, iter: 200000\n"
     ]
    }
   ],
   "source": [
    "hist_loss = []\n",
    "for iteration in range(args_start_iter, cfg['max_iter']):\n",
    "    running_loss = 0.0\n",
    "\n",
    "    if iteration != 0 and (iteration % epoch_size == 0):\n",
    "        # reset epoch loss counters\n",
    "        loc_loss = 0\n",
    "        conf_loss = 0\n",
    "        epoch += 1\n",
    "\n",
    "    if args_warmup:\n",
    "        warmup_steps = 2000\n",
    "        if iteration < warmup_steps:\n",
    "            warmup_learning_rate(optimizer, iteration, warmup_steps)\n",
    "\n",
    "    cosine_period = int(cfg['lr_steps'][-1] / 100)\n",
    "    \n",
    "    if iteration > cfg['lr_steps'][0] and iteration % cosine_period == 0:\n",
    "        scheduler.step(iteration / cosine_period)\n",
    "    '''if iteration in cfg['lr_steps']:\n",
    "        step_index += 1\n",
    "        adjust_learning_rate(optimizer, args.gamma, step_index)'''\n",
    "\n",
    "    # load train data\n",
    "    try:\n",
    "        images, targets = next(batch_iterator)\n",
    "        # images, targets = data_loader\n",
    "    except StopIteration:\n",
    "        batch_iterator = iter(data_loader)\n",
    "        images, targets = next(batch_iterator)\n",
    "    except Exception as e:\n",
    "        print(\"Loading data exception:\", e)\n",
    "        print('ISSUEE :(')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if args_cuda:\n",
    "            images = Variable(images.cuda())\n",
    "            targets = [Variable(ann.cuda()) for ann in targets]\n",
    "        else:\n",
    "            images = Variable(images)\n",
    "            targets = [Variable(ann) for ann in targets]\n",
    "\n",
    "    # forward\n",
    "    out = net(images)\n",
    "    # backprop\n",
    "    optimizer.zero_grad()\n",
    "    loss_l, loss_c = criterion(out, targets)\n",
    "    loss = loss_l + loss_c\n",
    "    loss.backward()\n",
    "\n",
    "    nn.utils.clip_grad_norm_(net.parameters(), max_norm=20, norm_type=2)\n",
    "    optimizer.step()\n",
    "    loc_loss += loss_l.item()\n",
    "    conf_loss += loss_c.item()\n",
    "\n",
    "    running_loss += loss.item() * images.size(0)\n",
    "    hist_loss.append(running_loss)\n",
    "    \n",
    "    verbose_period = 1000\n",
    "    if iteration != 0 and iteration % verbose_period == 0:\n",
    "        print('timer: %.4f sec.' % (time.time() - t0))\n",
    "        t0 = time.time()\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        print('iter ' + repr(iteration) + ' || Loc loss: %.4f || Conf loss: %.4f || lr: %.6f' % (loc_loss / verbose_period, conf_loss / verbose_period, lr), end=' ')\n",
    "        loc_loss = 0\n",
    "        conf_loss = 0\n",
    "\n",
    "    if iteration != 0 and iteration % 5000 == 0:\n",
    "        print('Saving state, iter:', iteration)\n",
    "        torch.save(net, 'weights/ssd300_VOC_' + repr(iteration) + \"__\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + '.pth')\n",
    "torch.save(net, args_save_folder + '' + args_dataset + \"__\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + '.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net, args_save_folder + '' + args_dataset + '_net.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.lineplot(range(len(hist_loss)), hist_loss, ci=None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Anubhav\\anaconda3\\envs\\mypytorch\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sns.lineplot(range(len(hist_loss)), hist_loss, ci=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}